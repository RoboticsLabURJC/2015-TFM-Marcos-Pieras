\chapter{Software implementation}\label{cap.software}

In this chapter, we explain the algorithm that we have designed for solving the visual people tracking problem and its software implementation. 

\section{System overview}

The main contribution of this work is to develop a robust pedestrian tracking algorithm that utilizes both a neural network and does not miss the real time operation. To do so we use the tracking-by-detection framework, and combine people detection using a neural network, somehow slow but very accurate, and a regular feature tracking, very quick but prone to drift. 

The architecture of the system is summarized in the diagram \ref{software1}. Its input is a sequence of frames coming from a directory and its output is a CSV file. This file has the structure that the MOT's evaluation software requires. We divided the computing in two threads, the \textit{object detector thread} and the \textit{feature-based tracking thread}. The first is responsible of computing pedestrian detections using a neural network and sending them to the \textit{feature-based tracking thread}. The second one computes the tracking procedure frame to frame. In addition, when a new detection appears it is combined with the blobs of the tracker, this is called \textit{data association}. There are two different data associations in our algorithm. First, one in the feature-based tracking thread between the blobs in the previous frame and the new features in the current frame. Second, one that links the blobs of the feature-based tracking with the new pedestrians detected by the neural network in the object detector thread. This is different to the data association concept in the tracking-by-detection nomenclature. In our algorithm the data association, is the feature-based tracking module which links the detections with the blobs.


\begin{figure}[H]
\centering         
\includegraphics[width=12cm]{flows/bloque2.png}
\caption{Block diagram of the component.} \label{software1}
\end{figure}


%Temporally the system works as follows. The algorithm starts, first of all it launches the \textit{object detector thread}, which begins to compute the detections of the first frame. Meanwhile, the \textit{tracking thread} remains idle, waiting for the detections. When the object detector finishes, it sends the initial detection to the tracking thread and at the same time starts to compute the detections on the next new frame. When the tracking thread receives a detection, it begins to compute the tracking between frames. Considering that the threads are not synchronized, the object detection thread goes ahead of the tracking thread, it will have a detection which the tracking thread could not incorporate, thus, it saves the detections in a shared buffer. When the tracking thread arrives to a frame that has got a detection it has a mechanism to mix detection with the blobs, this detection has been buffered, what it is called data association module, in addition it has a person re-identification module, to solve some possible identity incongruities, then it will continue computing the tracking.
%We can observe this temporal process in the next figure \ref{software2}, when $T$ represents a temporal step. When the object detector thread finishes computing all the detections it will \textit{die} and when the tracking thread process all the frame it dies too and the component it.


The system works as follows. When the algorithm starts, first of all it launches the \textit{object detector thread}, which begins to compute the detections on the first frame. Meanwhile, the \textit{feature-based tracking thread} remains idle, waiting for the initial detections. When the object detector finishes for the first time, it sends the initial detection to the \textit{feature-based tracking thread} through a buffer and at the same time starts to compute the detections on the next frame. When the \textit{feature-based tracking thread} receives from the buffer, it begins to compute the tracking between frames. Thus, the object detector thread introduces a suspension on the tracking system and in order to synchronized them we need to introduce a controlled delay on the \textit{feature-based tracking thread}. With this controlled delay, we are able to mix the \textit{feature-based tracking} estimation with its correspondent temporal detection. We can not process all the frames with the neural network, it would delay to much the system, the neural network is able to throw a detection every $30$ frames, so we sample the sequences of frames every $30$ frames. The process of mix the detections with the tracking estimation it is called data association module, in addition it has a person re-identification module, to solve some possible identity incongruities.

We can observe this temporal process in the next figure \ref{software2timin}, when $T$ represents a temporal step. With this controlled delay we are able to mix the detections of the neural network and do not miss the real time operation. When the object detector thread finishes computing all the detections it will \textit{die} and when the tracking thread processed all the frames it dies and the component too, it has finished the work. 


\begin{figure}[H]
\centering         
\includegraphics[width=\textwidth]{timesDiagram/timingVersion(1).png}
\caption{Timing of the component.} \label{software2timin}
\end{figure}


In the figure \ref{introTracking3} there is a flow chart of the algorithm. The object detector thread reads images, processes the forward pass of the neural network and saves the detections in the shared buffer, it repeats this sequence until it has processed all the periodic detections. In another hand, the main tread activates the object detector thread and waits till it gets the first detection, after this, it starts the tracking algorithm. It reads the images and computes the motion of all the regions of interest. However, at beginning of each cycle it checks whether it has got newer detection coming from the \textit{object detector thread} to mix it in.

\begin{figure}[H]
	
\centering

\subfigure[Tracking thread.]{\includegraphics[width=5cm]{flows/arc.png}}
\subfigure[Object detector thread.]{\includegraphics[width=5cm]{flows/detetc.png}}\\


\caption{Flow chart of the system.}
\label{introTracking3}
\end{figure}

We represent each person with a bounding box, in this bounding box we extract some features and compute how they move through the frames. Based on the movement of those features we will infer the movement of the bounding box, therefore, the movement of the person. This process is represented in the figure \ref{systemintro2}.


\begin{figure}[H]
		
\centering

\subfigure[Detection.]{\includegraphics[width=3cm]{BoundingBox3/bounding1.png}}
\subfigure[Points.]{\includegraphics[width=3cm]{BoundingBox3/ppints1.png}}
\subfigure[Displacement.]{\includegraphics[width=3cm]{BoundingBox3/arrow1.png}}\\
\caption{Image and motion vectors of a moving camera sequence.}
\label{systemintro2}
\end{figure}




With this approach we accomplish to join two different technologies, we can exploit their benefits and reduce their drawbacks. We can get an accurate detection every $30$ frames and in between, we link those detections with the feature-based tracking module. In this way, we reduce the fragility of the tracking, which without the detections it tends to miss feature points and cause a drift of the estimation. With the periodic neural-net based detection, that corrects that drift. In addition, we compensate the slowness of the neural-net based detection with the speed of the feature tracking.



Next we explain each part in detail.

\section{Object detector thread}


We compute the pedestrian detector based on a CNN. This type of systems is very accurate but slow. We are constrained by execution time of the chosen detector, it takes $0.92$ seconds for compute each detection, this allows us to get a new detection after $30$ frames. The \textit{object detector thread} reads images from the directory, processes the forward pass of the neural network and saves the detections in the shared buffer, it repeats this sequence until it has processed all the predefined list of frames. We ensured that we avoid the race condition between the threads by testing the worst scenario, when the tracking computation load is very low, this is when the tracking module has got only two blobs to process. In this case the controlled delay is enough to synchronized the threads. This process is summarized in Algorithm \ref{objectThresPseudo}.


\begin{algorithm}
%\caption{Object detection thread}\label{euclid}
\begin{algorithmic}[1]
\State \textbf{Input:} sequencesOfImages
\State \textbf{Output:} sharedVariable
\State $fpsRate = 30$
\State $numberFramesSequences = size(sequencesOfImages)$
\State $network = network.init()$
\State $listIndex = createList(FPS,numFramesSequences)$
\Procedure{Run}{}
\For {$indexImage$ in $listIndex$}
\State $image = read(indexImage)$
\State $detection = network.forward(image)$
\State $sharedVariable = detection$
\EndFor
\EndProcedure
\end{algorithmic}
\caption{Object detection thread}\label{objectThresPseudo}
\end{algorithm}

We selected the Single Shot multibox Detector (SSD) as object detector, because it has got the best balance between performance and speed, in section \ref{valdiation:det} we make a comparison of the available detectors. 


%This detector was trained with the union of the COCO and the VOC07 dataset and implemented on TensorFlow. Finally, in \ref{objectDetector1} we can observe the result of this step.

This detector uses the VGG network trained on ImageNet dataset for image classification purpose as feature extractor. The authors add the SSD layers to this feature extractor to build an object detector implemented on TensorFlow. To train the whole system, they only modify the weights of the SSD layers, the weights of the feature extractor are frozen, this process is called fine-tune. In this way we benefit the capabilities of the trained feature extractor and we need less example to train the whole network. The dataset for training the network is formed by the junction of the VOC07, VOC12 and COCO datasets. Although they are a generic datasets, the biggest category is the person instance. The network is trained for 12000 iterations and the weights are saved in a TensorFlow checkpoint. To use this detector we only need to load the weights in the initialization of the system. Finally, in the figure \ref{objectDetector1} we can observe the result of this step.

\begin{figure}[H]
\centering         
\includegraphics[width=\textwidth]{intro/deteccions.jpg}
\caption{Detections of the algorithm.} \label{objectDetector1}
\end{figure}

\section{Feature-based tracking thread}

The essence of this thread is the tracking module, called LK in the figure \ref{introTracking3}. It stands for Lucas-Kanade algorithm. This module computes for each blob its displacement by computing the displacement of the features points inside its bounding box. It will have one blob for each person detected and it will update the position of each blob as it evolves and moves in the image flow. The blobs are also called trackets.

\subsection{Feature extraction}

For extracting the features, we use the OpenCV routine \texttt{goodFeaturesToTrack()}, this function determines strong corners on an image, according the Shi-Tomasi method. Its parameters and values are the following:
 
\begin{itemize}

\item \texttt{image}, input image

\item \texttt{maxCorners}, maximum number of corners to return. If more corners than this maximum are found, the strongest of them are returned. We set this value experimentally to 60.
\item \texttt{qualityLevel}, the minimal accepted quality of image corners. We set this value experimentally to 0.1.
\item \texttt{minDistance}, minimum possible Euclidean distance between the returned corners. We set this value experimentally to 2.
\item \texttt{mask}, optional region of interest. Not used.
\item \texttt{blockSize}, Size of an average block for computing a derivative covariation matrix over each pixel neighborhood. We set this value experimentally to 7.
\item \texttt{useHarrisDetector},  Parameter indicating whether to use a Harris detector. Not used.
\item \texttt{k},  Free parameter of the Harris detector. Not used.

\end{itemize}

We applied an equalization transformation to the image before the feature extraction, to obtain more high contrast points. In the experiment described \ref{exper:validation} we performed a comparison of several preprocessing techniques. We can observe the results of this feature extraction in the figure \ref{solution2} and for all the blobs it looks like figure \ref{solution3}.


\begin{figure}[H]
\centering         
\includegraphics[width=3cm]{implementation/pointsEQU.jpg}
\caption{Shi-Tomasi points on a person.} \label{solution2}
\end{figure}

%
%\begin{algorithm}
%\caption{Object detection thread}\label{euclid}
%\begin{algorithmic}[1]
%\Procedure{Detection}{}
%%\State $A tracker prodcues a trajectory by tracking the point forward in time$
%%\State $network \gets \textit{patlen}$
%\State $network = network.init()$
%\State $FPS = 30$
%\State $FRAMES SEQUENCES = num of files(sequences)$
%\State $LIST_INDEXs = createList(FPS,FRAMES_SEQUENCES)$
%\For {(each object $LIST_INDEX$)}
%\State $image = read()$
%\State $detection = network.forward(image)$
%\State $sharedVariable = detection$
%\EndFor
%\EndProcedure
%\end{algorithmic}
%\end{algorithm}


\begin{figure}[H]
\centering         
\includegraphics[width=\textwidth]{intro/pounts.jpg}
\caption{Blobs with their feature points.} \label{solution3}
\end{figure}

 
%\begin{algorithm}
%\caption{Forward-backward method}\label{euclid}
%\begin{algorithmic}[1]
%\Procedure{FB}{}
%\State $A tracker prodcues a trajectory by tracking the point forward in time$
%\State $i \gets \textit{patlen}$
%%\BState \emph{top}:
%\If {$i > \textit{stringlen}$} \Return false
%\EndIf
%\State $j \gets \textit{patlen}$
%%\BState \emph{loop}:
%\If {$\textit{string}(i) = \textit{path}(j)$}
%\State $j \gets j-1$.
%\State $i \gets i-1$.
%\State \textbf{goto} \emph{loop}.
%\State \textbf{close};
%\EndIf
%\State $i \gets i+\max(\textit{delta}_1(\textit{string}(i)),\textit{delta}_2(j))$.
%\State \textbf{goto} \emph{top}.
%\EndProcedure
%\end{algorithmic}
%\end{algorithm}

\subsection{Feature matching}


Once we have all the blobs with their feature points, we superimpose each current bounding box on the next frame. We extract the features of these ahead bounding boxes and match them with the previous ones. To computing the matching, we used the OpenCV's routine \texttt{calcOpticalFlowPyrLK()}. This function implements a sparse iterative version of the Lucas-Kanade optical flow with pyramids. And his parameters and values are the following:
 
\begin{itemize}

\item \texttt{prevImg}, first image.
\item \texttt{nextImg}, second image.
\item \texttt{prevPts}, vector of 2D points for which the flow needs to be found. 
\item \texttt{nextPts}, output vector of 2D points containing the calculated new positions of input features in the second image. 
\item \texttt{status}, output status vector, it tells you whether the optical flow has been found.  
\item \texttt{err}, each element of the vector is set to an error for the corresponding feature.
\item \texttt{winSize}, size of the search window at each pyramid level. We set this value experimentally to 15.
\item \texttt{maxLevel}, number of pyramid levels. We set this value experimentally to 4. 
\item \texttt{criteria}, parameter specifying the termination criteria of the iterative search algorithm. We set this value experimentally to 10 iterations.
\end{itemize}



For the example we can observe the matching between the points of consecutive frames in figure \ref{solution4}.

\begin{figure}[!]
\centering         
\includegraphics[width=0.3\linewidth]{implementation/matching.png}
\caption{Matched feature points.} \label{solution4}
\end{figure}




%--------------------------------

Once we have the correspondances between feature points of consecutive frames, we can compute the motion of the blob as the displacement of those features. After this step we have a bunch of motion vectors, but some vectors in the bounding box do not belong to the pedestrian, and if we do not erase them, they will contribute to the motion computation. Usually these points belong to the static elements of the scene, like the floor or urban furniture, these points in terms of motion between subsequent frames will be very low or almost static. We can observe this fact plotting the displacement of these points and drawing them in the image, we can observe it at figure \ref{motion2}, the red points in the plot and in the image are considered static and the green ones are not. So, we erase the points with a displacement in both dimensions smaller than a threshold. We set this value experimentally to 1.0. If all the points are static, the algorithm considers the blob as static and does not upload its estimation.  


\begin{figure}[H]
		
\centering

\subfigure[Image feature points.]{\includegraphics[width=2.5cm]{implementation/reejctMore.png}}
\subfigure[Displacement graphics.]{\includegraphics[width=13cm]{implementation/filterVeloPoints.png}}\\
\caption{Image and motion vectors.}
\label{motion2}
\end{figure}

But, this behavior will only work on sequences where the camera is fixed, in sequences produced by a moving camera it will not work, we can observe the plot of displacement vectors on a sequence acquired by a moving camera in \ref{motion3}. 

\begin{figure}[H]
		
\centering

\subfigure[Image feature points.]{\includegraphics[width=2.5cm]{implementation/foto004.png}}
\subfigure[Displacement graphics.]{\includegraphics[width=13cm]{implementation/figurePoints.png}}\\
\caption{Image and motion vectors of a moving camera sequence.}
\label{motion3}
\end{figure}


\subsection{Blob matching}


Once we have the correct correspondances and erased those static feature points, we compute the displacement of the blob as the median of all of displacements in each dimension. We also compute the change of the scale of the blob, it is computed as follows: for each matched feature point, a ratio between the current feature point position and the next feature point position, is computed. Thus, the bounding box scale change is defined as the median over these ratios. In the figure \ref{solution5} we can observe a representation of this displacement for each blob.


\begin{figure}[H]
\centering         
\includegraphics[width=\textwidth]{intro/alcover2.png}
\caption{Displacement of each blob.} \label{solution5}
\end{figure}


With this displacement and the change in scale we can update the predicted position of the blob in the next frame. In the figure \ref{solution4} we can observe the previous estimation and the new one. At this point we have solved the data association problem in the tracking-by-detection nomenclature.


\begin{figure}[H]
\centering         
\includegraphics[width=\textwidth]{intro/deteccionsBLOB.jpg}
\caption{Uploaded estimation.} \label{solution4}
\end{figure}

%--------------

%In \ref{trackingModule} there is a pseudocode that summarizes these steps, this pseudocode given two temporal blobs, \textit{Blob1} and \textit{Blob2} computes its displacement in each coordinate and its change in scale. At this point, we have an implementation of the tracking algorithm given a set of bounding boxes.

%In \ref{trackingModule} there is a pseudocode that summarizes these steps, this pseudocode given the current blob computes the new estimation, \textit{Blob1} and \textit{Blob2}, respectively. At this point, we have an implementation of the tracking algorithm given a set of bounding boxes.
%
%
%In \ref{trackingModule} there is a pseudocode that summarizes these steps, with the current blob and the initial estimation of the next blob, computed as the superimposition of the bounding box on the next frame. This pseudocode computes the new estimation of the blob. At this point, we have an implementation of the tracking algorithm given a set of bounding boxes.


In \ref{trackingModule} there is a pseudocode that summarizes these steps, with the current blob, \textit{blob1} and the superimposition of the bounding box on the next frame, what is called \textit{Blob2}, it computes the new estimation of the blob. At this point, we have an implementation of the tracking algorithm given a set of bounding boxes.



\begin{algorithm}
%\caption{Object detection thread}\label{euclid}
\begin{algorithmic}[1]
\State \textbf{Input:} Blob1,Blob2 
\State \textbf{Output:} displacementX,displacementY,diffScale
%\Procedure{RUN}{}
\State $blob1Equ = equalize(Blob1)$
\State $features1 = cv2.goodFeaturesToTrack(blob1Equ)$
\State $blob2Equ = equalize(Blob2)$
\State $features2 = cv2.calcOpticalFlowPyrLK(blob1Equ,blob2Equ,features1)$
\State $displacement = features2-features1 $
\If{displacement $>$ threshold}
\State delete(displacement)
\EndIf
\State $displacementX = median(displacement[:,0]) $
\State $displacementY = median(displacement[:,1]) $
\State $diffScale = median(features2/features1) $
%\EndProcedure
\end{algorithmic}
\caption{LK module}\label{trackingModule}
\end{algorithm}


\vspace{2cm}


Nevertheless, the feature tracking is very sensitive to crossings between pedestrians, although the bounding boxes are fitted to body of the pedestrian it could include points belonging to another pedestrian. We can observe this event in figure \ref{traccs}. In this case the movement estimation is wrong and eventually the pedestrian will not be embedded by the bounding box.


\begin{figure}[H]
\centering         
\includegraphics[width=0.9\linewidth]{velocidadas/mateuPont.png}
\caption{Tracking failure.} \label{traccs}
\end{figure}


So, we need a mechanism to detect these failures. Therefore we studied how the motion algorithm behaves in these situations. When it has got a trajectory without crossing with other pedestrian, the vertical and horizontal displacements roughly behave like a damping sine wave (if it goes away of the camera)  or amplified sine wave ( if it goes closer to the camera ). But when it has got an interference with another pedestrian, it has an steep change in that wave. We can measure that change as the differences between the current displacement and the previous one normalized by current displacement. If this value overtakes a threshold, we consider then that the tracker has lost a track. We can observe this process in the next figure \ref{traccs23}, it belongs to previous trajectory shown at Figure \ref{traccs} 


\begin{figure}[H]
\centering         
\includegraphics[width=0.9\linewidth]{velocidadas/bad_threshold.png}
\caption{Tracking failure displacements.} \label{traccs23}
\end{figure}


In contrast, when it does not cross with another pedestrian, the displacement does not get disrupted, then the normalized differences with the previous displacement gets a low value. We can observe this process in figure \ref{motion2nocoorrect}. We set a threshold to notice this interference and delete this bounding box. We delete them from the current tracking execution, but we save the bounding box for following processings.

\begin{figure}[H]
		
\centering

\subfigure[Trajectory.]{\includegraphics[width=15cm]{velocidadas/tomeuPont.png}}\\
\subfigure[Plots movement.]{\includegraphics[width=16cm]{velocidadas/good_treshold.png}}\\
\caption{Wrong trajectory.}
\label{motion2nocoorrect}
\end{figure}





\section{Data association with detected pedestrians}

Once we computed the trajectories, in the next iteration we might have to add a detection, so we need a module to combine these trajectories with detections coming from the neural net. Thus, for each pedestrian we distinguish three situations:

\begin{itemize}



\item \textbf{Situation 1}, the tracket has got a nearby detection, then the detection replaces the tracket bounding box. This is what is called spatio-temporal constraint.

\item \textbf{Situation 2}, the tracket has not got a nearby detection, then the tracking of the bounding box, that is the blob continues.

\item \textbf{Situation 3}, the detected pedestrian does not have any close blob. In this case we need to decided whether this pedestrian is new in the scene or it has been seen before ( it is a lost tracket ).

\end{itemize}

We can observe the procedure for first and second situations in the figure \ref{data1}. In green colour we can observe the detections and in blue colour the trackets. We defined \textit{nearby} as the distance between the centres of the bounding boxes, this distance has to be lower than a threshold to be considered nearby. 

\begin{figure}[hptb]
\centering         
\includegraphics[width=\textwidth]{lucasKanade/dataAssociation.jpg}
\caption{Spatio-temporal data association.} \label{data1}
\end{figure}


To maintain the identity of the pedestrian we need a method to compare missed trackets those with no associated detections. We decided to solve it with deep learning techniques. In particular, a Siamese convolutional neural network, with In-network architecture in figure \ref{saimss} we can observe a diagram and the feature dimension of each layer. This network concatenates two blobs and computes a probability to belong to the same identity. It has got 6 convolutional layer and 1 fully connected layer, it was implemented on Keras with a Theano backend. In section \ref{exper:entrenar} we explain why we selected this architecture and how we trained it.



\begin{figure}[!]
\centering         
\includegraphics[width=\textwidth]{timesDiagram/network6conv.png}
\caption{Siamese network: In-network.} \label{saimss}
\end{figure}


For each detection we compare with all the missed trackets, if the maximum value of this  comparison is greater than a threshold we assign it to that identity. If it is not we consider that detection as a new identity. We can observe this data association process in Algorithm \ref{dataAssoc22}.



\begin{algorithm}
\caption{Data Association}\label{dataAssoc22}
\begin{algorithmic}[1]
\State \textbf{Input:} listBlobs,listDetections,listLostBlobs 
\State \textbf{Output:} listBlobs
\Procedure{Run}{}
\State $siamese = siamese.init()$
\For {$i$ in $listBlobs$}
\State $distance = euclideanDistance(blob[i],listDetections)$
\State $distanceOrdered = argmin(distance)$
\State $\%$ Situation1
\If{distanceOrdered[0] $<$ threshold}
\State newBlobs.append(listDetections[idx])
\State delete(listDetections[idx])
\State $\%$ Situation2
\Else
\State newBlobs.append(listBlobs[i])
\EndIf
\EndFor
\State $\%$ Situation3
\For {$i$ in $listDetectionsNotAssigned$}
\State $similarity = siamese.forward(listLostBlobs,DetectionsNotAssigned[i])$
\State $similarityOrdered = argmin(similarity)$
\If{similarityOrdered[0] $<$ threshold}
\State newBlobs.append(listDetectionsNotAssigned[i])
\State delete(listDetectionsNotAssigned[i])
\Else
\State newBlobs.append(listDetectionsNotAssigned[i])
\EndIf
\EndFor
\EndProcedure
\State listBlobs=newBlobs
\end{algorithmic}
\end{algorithm}


