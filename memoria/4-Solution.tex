\chapter{Software implementation}\label{cap.software}

In this chapter, we explain the algorithm that we have designed for solving the visual people tracking problem and its software implementation. 

\section{System overview}

The main contribution of this work is to develop a tracking algorithm that utilizes both a neural network and do not miss the real time operation. To do so we use the tracking-by-detection framework, and combine people detection using a neural network, somehow slow but very accurate, and link these detections with a regular feature tracking, very quick but prone to drift. 

The architecture of the system is summarized in the diagram \ref{software1}. Its inputs is a sequence of frames and its output is a CSV file. This file has the structure of the MOT's evaluation software requires. We divided the computing in two threads, the \textit{object detector thread} and the \textit{tracking thread}. The first is responsible of given a frame compute their pedestrian detections and send them to the tracking thread, and the second one, the tracking thread, given a detection it computes the tracking procedure, in addition, when a new detection appears it combines with the tracker, this is called data association, and finally it saves the results of the algorithm. We called data association to the process of combine a detection with the trackets, this is different to the data association concept in the tracking-by-detection nomenclature. In our algorithm the data association, like in the tracking-by-detection nomenclature, is the tracking module which links the detections. 



\begin{figure}[H]
\centering         
\includegraphics[width=12cm]{flows/bloque.png}
\caption{Block diagram of the component.} \label{software1}
\end{figure}


Temporally the system works as follows, when the algorithm starts, first of all it launches the object detector thread, it begins to compute the detections of the first frame. Meanwhile, the tracking thread remains idle, waiting for the detections. When the object detector finishes, it sends the detection to the tracking thread and at the same time starts to compute the detections of a predefined new frame. When the tracking thread receives a detection, it begins to compute the tracking between frames. Considering that the thread are not synchronized, the object detection thread goes ahead of the tracking thread it will have a detection which the tracking thread could not incorporate, thus, it saves the detections in a shared buffer. When the tracking thread arrives to a frame that has got a detection it has a mechanism to mix detection with trackets, what it is called data association module, in addition it has a person re-identification module, to solve some possible identity incongruities, then it will continue computing the tracking. We can observe this temporal process in the next figure \ref{software2}, when $T$ represents a temporal step. When the object detector thread finishes computing all the detections it will \textit{die} and when the tracking thread process all the frame it dies too and the component it 


\begin{figure}[H]
\centering         
\includegraphics[width=14cm]{timesDiagram/timing3.png}
\caption{Timing of the component.} \label{intro1}
\end{figure}


In the figure \ref{introTracking3} there is a flow chart of the algorithm and it summarizes as follows: The object detector thread reads images, processes the forward pass of the neural network and saves the detections in the shared buffer, it repeats this sequence until it has processed all the predefined list of frames. In another hand, the main tread, activates the object detector thread and waits till it gets the first detection, after this, it starts the tracking algorithm. It reads the images and computes the motion of all the regions of interest. However, at beginning of each cycle it checks whether it has newer detection to mix in. This is the main operating mode of the algorithm and it summarizes in the figure \ref{system1}. 

\begin{figure}[H]
	
\centering

\subfigure[Tracking thread.]{\includegraphics[width=5cm]{flows/arc.png}}
\subfigure[Object detector thread.]{\includegraphics[width=5cm]{flows/detetc.png}}\\


\caption{Flow chart of the system.}
\label{introTracking3}
\end{figure}

We represent each person with a bounding box, in this bounding box we extract some features and compute how they movement through the frames, based on the movement of those features we will infer the movement of the bounding box, therefore, the movement of the person.


\begin{figure}[H]
		
\centering

\subfigure[Detections.]{\includegraphics[width=3cm]{BoundingBox3/bounding1.png}}
\subfigure[Points.]{\includegraphics[width=3cm]{BoundingBox3/ppints1.png}}
\subfigure[Displacements.]{\includegraphics[width=3cm]{BoundingBox3/arrow1.png}}\\
\caption{Image and motion vectors of a moving camera sequence.}
\label{motion3}
\end{figure}




With this approach we accomplish to join two opposite technologies, we can exploit their benefits and reduce their drawbacks. We can get an accurate detection every $30$ frames and in between, we link those detections with the tracking by features module. In this way, we reduce the fragility of the tracking, it tends to miss feature points and cause a drift of the estimation, with a new detection, that corrects that drift. In addition, we compensate the slowness of the detection with the speed of the feature tracking.



Next we explain each part in detail.

\section{Object detector thread}



As we stated previously we compute the pedestrian detector based on a CNN. This types of systems are very accurate but slow. We are constraint by its execution time, it takes $0.92$ seconds for compute each detection, this allows us to get a new detection after $30$ frames. 

\begin{algorithm}
\caption{Object detection thread}\label{euclid}
\begin{algorithmic}[1]

\State $FPS = 30$
\State $numFramesSequences = size(sequences)$
\State $network = network.init()$
\State $listIndex = createList(FPS,numFramesSequences)$
\Procedure{Run}{}
\For {$indexImage$ in $listIndex$}
\State $image = read(indexImage)$
\State $detection = network.forward(image)$
\State $sharedVariable = detection$
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

We selected the Single shot multibox detector as object detector, because it has the best balance between performance and speed, in section \ref{valdiation:det} we make a comparison of the available detectors. Finally, in \ref{objectDetector1} we can observe the result of this step.


\begin{figure}[H]
\centering         
\includegraphics[width=10cm]{intro/deteccions.jpg}
\caption{Detections of the algorithm.} \label{objectDetector1}
\end{figure}


\section{Tracking thread}




The essence of the tracking thread it is the tracking module, called LK in the figure \ref{introTracking3}. It stands for Lucas-Kanade algorithm, due it is the method that we used in this work. This module for each detection computes its displacement by computing the displacement of the features inside this bounding box. 



\begin{algorithm}
\caption{Tracking thread}\label{euclid}
\begin{algorithmic}[1]

\State $FPS = 30$
\State $numFramesSequences = size(sequences)$
\State $network = network.init()$
\State $listIndex = createList(FPS,numFramesSequences)$
\Procedure{Run}{}
\For {$indexImage$ in $listImages$}
\State $image = read(indexImage)$
\State $detection = network.forward(image)$
\State $sharedVariable = detection$
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}


\subsection{Feature extraction}

It computes for each ROI at each step. For extracting the features, we use the OpenCV routine \texttt{goodFeaturesToTrack()}, this function determines strong corners on an image, according the Shi-Tomasi method. His parameters are the following:
 
\begin{itemize}

\item \texttt{image}, input image

\item \texttt{maxCorners}, maximum number of corners to return. If there are more corners than are found, the strongest of them is returned.
\item \texttt{qualityLevel}, parameter characterizing the minimal accepted quality of image corners.
\item \texttt{minDistance}, minimum possible Euclidean distance between the returned corners.
\item \texttt{mask}, optional region of interest.
\item \texttt{blockSize}, Size of an average block for computing a derivative covariation matrix over each pixel neighborhood.
\item \texttt{useHarrisDetector},  Parameter indicating whether to use a Harris detector.
\item \texttt{k},  Free parameter of the Harris detector.

\end{itemize}

We applied an equalization transformation to the image before the feature extraction, to obtain more high contrast points, in \ref{exper:validation} we perform a comparision of several preprocessing techniques. We can observe the results of this process in the figure \ref{solution2} and for all the ROIs it looks like \ref{solution3}.


\begin{figure}[H]
\centering         
\includegraphics[width=3cm]{implementation/pointsEQU.jpg}
\caption{Shi-Tomasi points on a person.} \label{solution2}
\end{figure}

%
%\begin{algorithm}
%\caption{Object detection thread}\label{euclid}
%\begin{algorithmic}[1]
%\Procedure{Detection}{}
%%\State $A tracker prodcues a trajectory by tracking the point forward in time$
%%\State $network \gets \textit{patlen}$
%\State $network = network.init()$
%\State $FPS = 30$
%\State $FRAMES SEQUENCES = num of files(sequences)$
%\State $LIST_INDEXs = createList(FPS,FRAMES_SEQUENCES)$
%\For {(each object $LIST_INDEX$)}
%\State $image = read()$
%\State $detection = network.forward(image)$
%\State $sharedVariable = detection$
%\EndFor
%\EndProcedure
%\end{algorithmic}
%\end{algorithm}


\begin{figure}[H]
\centering         
\includegraphics[width=10cm]{intro/pounts.jpg}
\caption{Detections with feature points.} \label{solution3}
\end{figure}

 
%\begin{algorithm}
%\caption{Forward-backward method}\label{euclid}
%\begin{algorithmic}[1]
%\Procedure{FB}{}
%\State $A tracker prodcues a trajectory by tracking the point forward in time$
%\State $i \gets \textit{patlen}$
%%\BState \emph{top}:
%\If {$i > \textit{stringlen}$} \Return false
%\EndIf
%\State $j \gets \textit{patlen}$
%%\BState \emph{loop}:
%\If {$\textit{string}(i) = \textit{path}(j)$}
%\State $j \gets j-1$.
%\State $i \gets i-1$.
%\State \textbf{goto} \emph{loop}.
%\State \textbf{close};
%\EndIf
%\State $i \gets i+\max(\textit{delta}_1(\textit{string}(i)),\textit{delta}_2(j))$.
%\State \textbf{goto} \emph{top}.
%\EndProcedure
%\end{algorithmic}
%\end{algorithm}

\subsection{Feature matching}


Once we have all the detections with their feature points, we can compute its displacement, to do so,  we used the OpenCV's routine \texttt{calcOpticalFlowPyrLK()}, this function implements a sparse iterative version of the Lucas-Kanade optical flow with pyramids. And his parameters are the following:
 
\begin{itemize}

\item \texttt{prevImg}, first image.
\item \texttt{nextImg}, second image.
\item \texttt{prevPts}, vector of 2D points for which the flow needs to be found. 
\item \texttt{nextPts}, output vector of 2D points containing the calculated new positions of input features in the second image. 
\item \texttt{status}, output status vector, it tells you whether the flow has been found.  
\item \texttt{err}, each element of the vector is set to an error for the corresponding feature.
\item \texttt{winSize}, size of the search window at each pyramid level. 
\item \texttt{maxLevel}, number of pyramid levels.  
\item \texttt{criteria}, parameter, specifying the termination criteria of the iterative search algorithm.
\end{itemize}



For the example's person we can observe the matching between the points of consecutive frames in figure \ref{solution4}.

\begin{figure}[hptb]
\centering         
\includegraphics[width=0.3\linewidth]{implementation/matching.png}
\caption{Matched feature points.} \label{solution4}
\end{figure}


%--------------------------------

After this step we have a bunch of motion vectors, but some vectors in the bounding box do not belong to the pedestrian, and if we do not erase it will contribute to the motion computation. Usually this points belong to the static elements of the scene, like the floor or urban furniture, this points in terms of motion between subsequent frames will be very low or almost static. We can observe this fact plotting the displacement of this points and drawing it in the image, we can observe it at \ref{motion2}. So, we erase the points with a displacement smaller than a threshold. 

\begin{figure}[H]
		
\centering

\subfigure[Image points.]{\includegraphics[width=2.5cm]{implementation/reejctMore.png}}
\subfigure[Displacement graphics.]{\includegraphics[width=13cm]{implementation/filterVeloPoints.png}}\\
\caption{Image and motion vectors.}
\label{motion2}
\end{figure}

But, this behavior will only work on sequences where the camera is fixed, in sequences produced by a moving camera it will not work, we can observe the plot of displacement vectors on a sequence acquired by a moving camera in \ref{motion3}. 

\begin{figure}[H]
		
\centering

\subfigure[Image points.]{\includegraphics[width=2.5cm]{implementation/foto004.png}}
\subfigure[Displacement graphics.]{\includegraphics[width=13cm]{implementation/figurePoints.png}}\\
\caption{Image and motion vectors of a moving camera sequence.}
\label{motion3}
\end{figure}


Once we have the correct correspondaces, we compute the displacement as the median of all of them in each dimension. We also compute the change of the scale of the ROI, it is computed as follows: for each pair of points, a ratio between current point distance and previous point distance is computed, bounding box scale change is defined as the median over these ratios. In the figure \ref{solution5} we can observe a representation of this displacement for each ROI. At this point we have solved the data association problem in the tracking-by-detection nomenclature.

\begin{figure}[H]
\centering         
\includegraphics[width=10cm]{intro/alcover2.png}
\caption{Displacement of each ROI.} \label{solution5}
\end{figure}


%--------------



At this point, we have an implementation of the tracking algorithm given a set of bounding boxes. Nevertheless, the feature tracking is very sensitive to crossings between pedestrian, although the bounding boxes are fitted to body of the pedestrian it could include points belonging to another pedestrian, in this case the movement estimation is wrong and eventually the pedestrian will not be embedded by the bounding box. We can observe this event in figure \ref{traccs}.


\begin{figure}[H]
\centering         
\includegraphics[width=0.9\linewidth]{velocidadas/mateuPont.png}
\caption{Tracking failure.} \label{traccs}
\end{figure}


So, we need a mechanism to detect these failures, therefore we studied how the motion algorithm behaves in these situations. When it has got a trajectory without crossing with other pedestrian, the vertical and horizontal displacement behave like a damping sine wave (if it goes away of the camera)  or amplified sine wave ( if it goes closer to the camera ). But when it has got an interference with another pedestrian, it has an steep change in that wave. We can measure that change as the differences between the current displacement and the previous one normalized by current displacement, if this value overtake a threshold, we consider that tracker as a lost track. We can observe this process in the next figure \ref{traccs23}, it belongs to previous trajectory \ref{traccs} 


\begin{figure}[H]
\centering         
\includegraphics[width=0.9\linewidth]{velocidadas/bad_threshold.png}
\caption{Tracking failure.} \label{traccs23}
\end{figure}


In contrast, when it does not cross with another pedestrian, the displacement does not get disrupt, then the normalized different with the previous displacement gets a low value, we can observe this process in figure \ref{motion2nocoorrect}. We set a threshold to notice this interference and delete this bounding box. We delete them from the current tracking execution, but we save the bounding box for following processings.

\begin{figure}[H]
		
\centering

\subfigure[Trajectory.]{\includegraphics[width=15cm]{velocidadas/tomeuPont.png}}\\
\subfigure[Plots movement.]{\includegraphics[width=16cm]{velocidadas/good_treshold.png}}\\
\caption{Wrong trajectory.}
\label{motion2nocoorrect}
\end{figure}













\section{Data association with detected pedestrians by the neural net}

Once we computed the trajectories, in the next iteration we might have to add a detection, so we need a module to combine these trajectories with detections. Thus, for each pedestrian we distinguish three situations:

\begin{itemize}



\item \textbf{Situation 1}, the tracket has got a nearby detection, then the detection replaces the tracket bounding box. This is what is called spatio-temporal constraint.

\item \textbf{Situation 2}, the tracket has not got a nearby detection, then the bounding box tracket continues.

\item \textbf{Situation 3}, the pedestrian has not got a tracket but has got a detection. In this case we need to decided whether this pedestrian is new in the scene or it has been seen before ( it is a lost tracket ).

\end{itemize}

We can observe the procedure for situations number one and two in the figure \ref{data1}, in green colour we can observe the detections and in blue colour the trackets. We defined nearby as the distance between the centres of the bounding boxes, this distance has to be lower than a threshold to be considered nearby. 

\begin{figure}[hptb]
\centering         
\includegraphics[width=12cm]{lucasKanade/dataAssociation.jpg}
\caption{Spatio-temporal data association.} \label{data1}
\end{figure}


To maintain the identity of the pedestrian we need a method to compare miss trackets with no associated detections, this is Situation 3. So, we decided to solve it with deep learning techniques. In particular, a Siamese convolutional neural network, with In-network architecture in \ref{saimss} we can observe a diagram of it. This networks concatenates two ROIs and computes a probability to belong to the same identity. In section \ref{exper:entrenar} we explain why we selected this architecture and how we trained it.



\begin{figure}[hptb]
\centering         
\includegraphics[width=16cm]{timesDiagram/network.png}
\caption{Siamese network: In-network.} \label{saimss}
\end{figure}


For each detection we compare with all the miss trackets, if the maximum value of this  comparision is greater than a threshold we assign it to that identity, if it is not we consider that detection as a new identity.


The tracking thread computes these steps for every frame in the sequence.



	\vspace{10pt}
	\begin{lstlisting}[frame=single]
	layer {
		name: "mnist"
		type: "Data"
		top: "data"
		top: "label"
		include {phase: TRAIN}
		transform_param {scale: 0.00390625}
		data_param {
			source: "examples/mnist/mnist_train_lmdb"
			batch_size: 64
			backend: LMDB }
	}
	\end{lstlisting}
	