\chapter{Experiments}\label{cap.experiments}


In this chapter we explain the validation experiments of our solution and characterize the quality of each module. Also, we explain several alternatives that we considered for each module.


\section{Validation experiments}\label{valdiation}

\subsection{Ordinary execution}



Using the code provided by the MOT16 challenge organization, we evaluate our solution. The evaluation procedure and dataset are explained in previous sections,  \ref{datasetracksEval} and \ref{datasetracks} respectively. The principal measure to compare the algoirhtms is the MOTA, this measure combines three error sources: false positives [FP], missed targets [FN] and identity switches [IDs]. Another measure is the track quality, this measure classify each trajectory as mostly tracked [MT], partially tracket [PT], and mostly lost [ML]. 


We show the results of our algorithm in the table \ref{tableResults}. We reach 10.8 of MOTA at 15.85 FPS, also around of $24 \%$  of the blobs are partially tracket.




%\begin{table}[H]
%\centering
%
%\resizebox{\textwidth}{!}{\begin{tabular}{l|llll|llll|lll|l}
%              &  \textbf{GT} & \textbf{MT} & \textbf{PT} & \textbf{ML} & \textbf{FP} & \textbf{FN} & \textbf{IDs} & \textbf{FM} & \textbf{MOTA} & \textbf{MOTP} & \textbf{MOTAL} & \textbf{FPS} \\
%\textit{v1}   &  517         & 12          & 180         & 325         & 13339       & 78998       & 827          & 1053        & 9.8           & 69.1          & 6.5            & 18.2         \\
%\textit{v2}  & 517         & 11          & 181         & 325         & 11212       & 75738       & 827          & 1056        & 9.7           & 67.3          & 6.1            & 9.0          \\
%\textit{v3}   & 517         & 3           & 127         & 387         & 13373       & 78999       & 618          & 936         & 10.8          & 70.3          & 7.5            & 15.85        \\
%\textit{SOTA} & 517         & 92          & 219         & 206         & 5333        & 86795       & 391          & -           & 49.3          & 79.0          & -              & 0.8         
%\end{tabular}}
%\caption{Results algorithm global.}
%\label{tableResults}
%\end{table}


\begin{table}[H]
\centering

\resizebox{\textwidth}{!}{\begin{tabular}{l|llll|lll|ll|l}
              &  \textbf{GT} & \textbf{MT} & \textbf{PT} & \textbf{ML} & \textbf{FP} & \textbf{FN} & \textbf{IDs}  & \textbf{MOTA} & \textbf{MOTP}  & \textbf{FPS} \\
\textit{Our algorithm}   & 517         & 3           & 127         & 387         & 13373       & 78999       & 618             & 10.8          & 70.3                & 15.85        \\
\end{tabular}}
\caption{Results of our algorithm.}
\label{tableResults}
\end{table}





%According to this results, we decided to do not use the forward-backward method, due for its high computation demands and its limited improvement. Also, we observe the usage of a mechanism of re-identification increment the MOTA performance, so the overall performance of the detector, with the cost of increase the running time. This improvement come from the reduction in around $24 \%$ the identity switching (ID's) parameter. From this statistical comparision we discard the forward-backward method and it is worth to use the person re-identification module.

In addition, we show the results for each sequences, we can observe the results in the table \ref{tableResultsSequences}. 


%\begin{table}[]
%\centering
%\caption{My caption}
%\begin{tabular}{lllll}
%Name        & FPS & Resolution & Density & Description                        \\ \hline
%\textit{02} & 30  & 1920x1080  & 29.7    & Fixed camera, low angle            \\
%\textit{04} & 30  & 1920x1080  & 45.3    & Fixed camera, elevated viewpoint   \\
%\textit{05} & 14  & 640x480    & 8.1     & Moving camera, low angle           \\
%\textit{09} & 30  & 1920x1080  & 10      & Fixed camera, low angle            \\
%\textit{10} & 30  & 1920x1080  & 18.8    & Moving camera, low angle           \\
%\textit{11} & 30  & 1920x1080  & 10.2    & Moving camera, low angle           \\
%\textit{13} & 25  & 1920x1080  & 15.3    & Moving camera, elevated  viewpoint
%\end{tabular}
%\label{my-label}
%\end{table}


\begin{table}[!]
\centering

\resizebox{\textwidth}{!}{\begin{tabular}{l|llll|lll|ll|l}
                & \textbf{GT} & \textbf{MT} & \textbf{PT} & \textbf{ML} & \textbf{FP} & \textbf{FN} & \textbf{IDs}  & \textbf{MOTA} & \textbf{MOTP}  & \textbf{FPS} \\
\textit{02}     & 54          & 0           & 13          & 41          & 2181        & 15526       & 113                  & 0.1           & 67.1           & 9.02         \\
\textit{04}     & 83          & 0           & 41          & 42          & 5495        & 33980             & 290         & 16.6          & 71.1          &  12.3         \\
\textit{05}              & 125         & 3           & 43          & 79          & 28571       & 4713              & 109         & -12.2         & 67.8          & 17.94        \\
\textit{09}              & 25          & 1           & 19          & 5           & 932         & 3225              & 71          & 19.7          & 62           & 10.52        \\
\textit{10}              & 54          & 0           & 4           & 50          & 404         & 11647       & 81                    & 1.5           & 68.4                 & 14.23        \\
\textit{11}              & 69          & 0           & 16          & 53          & 948         & 7366        & 72                   & 8.6           & 71.4                 & 17.49        \\
\textit{13}              & 107         & 0           & 9           & 98          & 1315        & 10743       & 32                    & -5.6          & 67.1                  & 20.5         \\ \hline
\textit{Global} & 517         & 3           & 127         & 387         & 13373       & 78999       & 618                  & 10.8          & 70.3           & 15.85       
\end{tabular}}
\caption{Results algorithm by sequences.}
\label{tableResultsSequences}
\end{table}


The algorithm gets the best performance on sequences with a fixed camera from an elevated view point and a low angle recording with a high frame rate and close targets like sequences $4,9 $, and $11$. In the figure \ref{seq1} and \ref{seq2} we can observe a snapshot of these sequences.



\begin{figure}[H]
		
\centering

\subfigure[Our algorithm]{\includegraphics[width=7cm]{comparision/our04.jpg}}
\subfigure[Ground truth]{\includegraphics[width=7cm]{comparision/gt04.png}}\\
\caption{Comparision between our algorithm with MOT-04 ground truth.}
\label{seq1}
\end{figure}


\begin{figure}[H]
		
\centering

\subfigure[Our algorithm]{\includegraphics[width=7cm]{comparision/our09.jpg}}
\subfigure[Ground truth]{\includegraphics[width=7cm]{comparision/gt09.png}}\\
\caption{Comparision between our algorithm with MOT-09 ground truth.}
\label{seq2}
\end{figure}


In contrast, our algorithm struggles in sequences with low frame rate and resolution like sequences $5$ and when the targets are away from the camera like $13$. In the figure \ref{seq3} and \ref{seq4} we can observe a snapshot of these sequences.


\begin{figure}[H]
		
\centering

\subfigure[Our algorithm]{\includegraphics[width=7cm]{comparision/our13.jpg}}
\subfigure[Ground truth]{\includegraphics[width=7cm]{comparision/gt13.png}}\\
\caption{Comparision between our algorithm with MOT-13 ground truth}
\label{seq3}
\end{figure}


\begin{figure}[H]
		
\centering

\subfigure[Our algorithm]{\includegraphics[width=7cm]{comparision/our05.jpg}}
\subfigure[Ground truth]{\includegraphics[width=7cm]{comparision/gt05.png}}\\
\caption{Comparision between our algorithm with MOT-05 ground truth}
\label{seq4}
\end{figure}




\subsection{Comparative}

%\begin{table}[H]
%\centering
%
%\resizebox{\textwidth}{!}{\begin{tabular}{l|llll|llll|lll|l}
%              &  \textbf{GT} & \textbf{MT} & \textbf{PT} & \textbf{ML} & \textbf{FP} & \textbf{FN} & \textbf{IDs} & \textbf{FM} & \textbf{MOTA} & \textbf{MOTP} & \textbf{MOTAL} & \textbf{FPS} \\
%\textit{Our}   & 517         & 3           & 127         & 387         & 13373       & 78999       & 618          & 936         & 10.8          & 70.3          & 7.5            & 15.85        \\
%\textit{SOTA} & 517         & 92          & 219         & 206         & 5333        & 86795       & 391          & -           & 49.3          & 79.0          & -              & 0.8         
%\end{tabular}}
%\caption{Results algorithm global.}
%\label{tableResults}
%\end{table}

We compare our algorithm with the MOT16 leaderboard \cite{motResults}, we only include the algorithms which belong to a research paper, in the table \ref{tableSOTatomeu} and the figure \ref{experimenComp} we can observe those results. We observe that these algorithms overtake our solution on the MOTA measure but we pass them in processing speed, even their processing speed parameter do not include the execution of their detector.

These algorithms are focused on solving the data association module from the tracking-by-detection paradigm, to do so they have access to the detection at each frame. They focus on how to link those detections and disdain the processing speed. Instead, we were focused on how to develop a tracking-by-detection with neural networks on real time.
 

\begin{table}[!]
\centering
\begin{tabular}{l|llll|lll|ll|l}
                 & \textbf{GT} & \textbf{MT} & \textbf{PT} & \textbf{ML} & \textbf{FP} & \textbf{FN} & \textbf{IDs} & \textbf{MOTA} & \textbf{MOTP} & \textbf{FPS} \\
\textit{DP\_NMS} & 517         & 28          & 169         & 320         & 1123        & 121578      & 972          & 32.2          & 76.4          & 212.6        \\
CEM              & 517         & 40          & 198         & 279         & 6837        & 114322      & 642          & 33.2          & 75.8          & 0.3          \\
\textit{SMOT}    & 517         & 22          & 253         & 242         & 17426       & 107552      & 3108         & 29.7          & 76.3          & 0.2          \\
\textit{LP2D}    & 517         & 44          & 211         & 262         & 5084        & 111163      & 915          & 35.7          & 75.8          & 49.3         \\
\textit{MDPNN}   & 517         & 72          & 287         & 215         & 2681        & 92856       & 774          & 47.2          & 75.8          & 1.0          \\
\textit{LMP}     & 517         & 98          & 222         & 197         & 8886        & 85487       & 852          & 48.8          & 79            & 0.5          \\
\textit{Our method}     & 517         & 3           & 127         & 387         & 13373       & 78999       & 618          & 10.8          & 70.3          & 15.85       
\end{tabular}
\caption{Comprarision with the MOT's results}
\label{tableSOTatomeu}
\end{table}



\begin{figure}[!]
\centering         
\includegraphics[width=16cm]{comparision/timeDAta.png}
\caption{Comparision with other algorithms.} \label{experimenComp}
\end{figure}



\section{Detection experiments}\label{valdiation:det}


For the choice of the detector we compared the detectors studied in the theoretical review \ref{trackingBounding} and we tested on the \textit{MOT16} dataset. In the figure \ref{experimDet1} we can observe the ROC curves of different detectors.



\begin{figure}[H]
\centering         
\includegraphics[width=0.9\linewidth]{evaluacionObject/dadas.png}
\caption{ROCs curves on the MOT16 dataset.} \label{experimDet1}
\end{figure}

In the figure \ref{experimDet2} we can observe the mean average precision against the time consumption.


\begin{figure}[H]
\centering         
\includegraphics[width=0.9\linewidth]{evaluacionObject/meanAverage2.png}
\caption{Mean average precision against time.} \label{experimDet2}
\end{figure}

With this information we can summarize the study of the detectors:

\begin{itemize}

\item \textbf{Faster-RCNN}, we used the TensorFlow implementation \cite{tensorObjectdte}, the original code required a Nvidia GPU. This repository include the Faster-RCNN model with ResNet as feature extraction and the ensemble model compound by Inception-ResNet. It scores $0.6872$ and $0.7081$ average precision with a time consumption of  $6.93$ and  $20.029$ seconds respectively.



\item \textbf{R-FCN}, the original code is not publicly available. We used the TensorFlow implementation \cite{tensorObjectdte}. It scores $0.6614$ average precision and $5.514$ seconds.


\item \textbf{YOLO}, we used the original and it scores $0.09$ average precision on the dataset, it takes $6$ seconds per image \cite{yoloDark}. 

\item \textbf{PVANET}, the code is not publicly available.

\item \textbf{SSD}, we tested several feature extractors with this model. Their score are the following: the SSD model with VGG as feature extractor. It scores $0.4612$ average precision and  takes $0.73$ seconds, SSD with Inception as feature extractor. It scores $0.3499$ average precision and  takes $0.73$ seconds, and SSD with MobileNet as feature extractor. It scores $0.2995$ average precision and  takes $0.198$ seconds. The original code \cite{ssdCode2} is not optimized for CPU execution, it takes about $3.5$ seconds and the Caffe framework does not allow to run it in a multithreading way, so we discarded it. The VGG version come from a particular developers \cite{ssdCode} and the Inception and MobileNet from TensorFlow organization \cite{tensorObjectdte}.


\end{itemize}

According to these results the object detector with the best balance between precision and time consumption is the SSD detector with VGG feature extractor. Detectors like SSD-Inception and SSD-MobileNet are really fast but their performance is $23 \%$ lower than the SSD with VGG. In contrast, RFCN is more accurate but it takes $700 \%$ more time than SSD with VGG.


The MOT organization provides a set of detections, they include FasterRCNN, DPM v5, and SPD \cite{spd}. We are not able to reproduce their results, because we can not access to the original code. In the figure \ref{experimDet3} we can observe those detections.




\begin{figure}[H]
\centering         
\includegraphics[width=0.9\linewidth]{evaluacionObject/motdetece.png}
\caption{ROCs curves on the MOT16 dataset.} \label{experimDet3}
\end{figure}


\section{Feature-based tracking experiments}\label{trackingsesad}



We started developing our tracking module with simple artificial objects like the one in the figure \ref{trs}. As soon we had got expertise we shift to much complex models like people. Finally, the last version of the tracking module was inspired by the well-known tracking algorithm \textit{MedianFlow} by Zalal et al\cite{medianFlow} with its correspondent implementation in Python\cite{medianFlowPython}.
 

\begin{figure}[H]
\centering         
\includegraphics[width=6cm]{tracker/scale1points.png}
\caption{Artificial object to start tracking.} \label{trs}
\end{figure}





%The tracking module is inspired by the well-known tracking algorithm \textit{MedianFlow} by Zalal et al\cite{medianFlow} with its correspondent implementation in Python\cite{medianFlowPython}.

The tracking using matching is based on the optical flow, explained in \ref{matchi}, it computes the new position through gradient descent in several frames. We will assume that the motion is pure translational. As we can observe in the sequences of frames \ref{experiTrack1} of the dataset, the pedestrians move in translation way in the image plane, so this assumption is achieved.

\begin{figure}[H]
\centering         
\includegraphics[width=0.9\linewidth]{changeCamera/tomeu.png}
\caption{Sequence of translational movement.} \label{experiTrack1}
\end{figure}

In contrast to the previous figure, we can observe the next figure \ref{experiTrack2} where the assumption of translation motion is not fulfilled ( this sequences does not belong to the used dataset, only showed to contrast the previous idea) and a translational assumption will failed.

\begin{figure}[H]
\centering         
\includegraphics[width=0.9\linewidth]{changeCamera/out2.png}
\caption{Sequence of no translational movement.} \label{experiTrack2}
\end{figure}

We tested other tracking by matching algorithms like MeanShift, but we discarded it due to his problems to tracking pedestrian with a messy background.






\subsection{Feature extraction improvement}\label{exper:validation}

The strength of the further processing depends on the quality and quantity of this features, so in other to improve both, we apply some prepreprocessing to the image. We tried several preprocessings techniques like sharpening, image contrast, median filter, and equalization. In \ref{experiTrack3}, we can observe the relation between number of points extracted and time consumption of those techniques.




\begin{figure}[H]
\centering         
\includegraphics[width=0.9\linewidth]{tracker/preprocesing.png}
\caption{Plot of different processings.} \label{experiTrack3}
\end{figure}

We realized that the best preprocessing in terms of speed and number of points, is to equalize the image. The computation is really simple, it only consists in equalize an histogram and apply that transformation to the image. It increases over $ 55 \%$ the number points in comparision to not applying it to the raw image. In the figure \ref{experiTrack4} we can observe the different number of features in the raw and in the equalized image.

\begin{figure}[H]
		
\centering

\subfigure[]{\includegraphics[width=3cm]{implementation/pointsSIN_EQU.jpg}}
\subfigure[]{\includegraphics[width=3cm]{implementation/pointsEQU.jpg}}\\
\caption{Comparision between feature extraction on raw and equalized image.}
\label{experiTrack4}
\end{figure}


\subsection{Matching module}

As we said previously, we used the Lukas-Kanade algorithm to get the displacement of the features but we implemented the same method used in \cite{medianFlow} too. The proposed method is based on so called forward-backward consistency assumption. That assumption consists in that correct tracking should be independent of the direction of time-flow.
Algorithmically, the assumption is exploited as follows. First, a tracker produces a trajectory by tracking the point \textit{forward} in time. Second, the point location in the last frame initializes a validation trajectory. The validation trajectory is obtained by \textit{backward} tracking from the last frame to the first one. Third, the two trajectories are compared and if they differ significantly, the forward trajectory is considered as incorrect. 
Figure \ref{experiTrack5} illustrates the method when tracking a point between two images. Point number $1$  is visible in both images and the tracker is able to localize it correctly. Tracking this point forward or backward results in identical trajectories. On the other hand, point number $2$ is not visible in the right image and the tracker localizes a different point. Tracking this point backward ends in a different location than the original one. We also implemented the forward method. We replaced for the tracking module in the algorithm.


\begin{figure}[H]
		
\centering

\subfigure[Image forward-backward.]{\includegraphics[width=7cm]{tracker/forwardBack.png}}
\subfigure[Scheme forward-backward.]{\includegraphics[width=7cm]{tracker/forwardBack2.png}}\\
\caption{Illustration forward backward error.}
\label{experiTrack5}
\end{figure}


In the table \ref{tableResultsTrack} we observe the result of the forward and the forward-backwward method. Both have the same MOTA but the forward-backward methods takes around 10 $\%$ more time.

\begin{table}[H]
\centering

\resizebox{\textwidth}{!}{\begin{tabular}{l|llll|llll|lll|l}
              &  \textbf{GT} & \textbf{MT} & \textbf{PT} & \textbf{ML} & \textbf{FP} & \textbf{FN} & \textbf{IDs} & \textbf{FM} & \textbf{MOTA} & \textbf{MOTP} & \textbf{MOTAL} & \textbf{FPS} \\
\textit{Forward}   & 517         & 3           & 127         & 387         & 13373       & 78999       & 618             & 10.8          & 70.3                & 15.85        \\
\textit{Forward-Backward}  & 517         & 11          & 181         & 325         & 11212       & 75738       & 827          & 1056        & 9.7           & 67.3          & 6.1            & 9.0          \\
  
\end{tabular}}
\caption{Comparision tracking modules.}
\label{tableResultsTrack}
\end{table}

\subsection{Tracking analysis}

In this part we realize a qualitative analysis of the tracking module. The main disadvantage of the feature-based tracking is the dependence on the quality of the features, this method needs blobs with high texture to accomplish a good tracking. Thus, a sequence of low resolution there are less points with these characteristics. Also, we have problems with people who wear low texture clothes or are away from the camera, as we can observe in figure \ref{Fails1}.


\begin{figure}[H]
		
\centering

\subfigure[High texture person.]{\includegraphics[width=5cm]{changeCamera/tomeuTetx.png}}
\subfigure[Low texture person.]{\includegraphics[width=5cm]{changeCamera/donaTetx.png}}
\subfigure[Far away person.]{\includegraphics[width=5cm]{changeCamera/foto004.png}}
\caption{Differences texture examples.}
\label{Fails1}
\end{figure}


Although a low frame rate could penalize the matching capabilities between frames, the pyramidal implementation of the Lucas-Kanade method solve it. In the figure \ref{fails2} we show the matching procedure of a blob belonging to a low frame rate sequence, and its result is correct.


\begin{figure}[H]
\centering         
\includegraphics[width=6cm]{lucasKanade/matchinBo.png}
\caption{Blob matching low frame rate sequence.} \label{fails2}
\end{figure}



\section{Data Association experiments}\label{exper:entrenar}


We solved the person reidentification problem with deep learning techniques, thus we tested several models:


\begin{itemize}


\item \textbf{Siamese network: Cost function}, this is based on the idea of deep learning as feature extractor and top layers as classifier. Two branches that share parameters process the images and classify it.

\item \textbf{Siamese network: In-network}, this is a mix of the previous models, where the information of the convolutional layers merges at some point before the classifier.

\item \textbf{Siamese network: Joint data input}, according to the literature this architecture gives the best results compared with the other topologies. The input of the network is a concatenation of the two images and the network process together.


\item \textbf{Feature extractor with cosine distance}, we used well-known architectures for image classification to extract features from the images and then compare those features with the cosine distance.

\item \textbf{Famous network fine-tuned}, we extract features for each image with a well-known architecture and merge it with a fully connected layer.

\end{itemize}

We can observe this architectures in the figure \ref{siameseData1}.

\begin{figure}[H]
		
\centering

\subfigure[Cost function.]{\includegraphics[width=2.7cm]{siamese/retall1.png}}
\subfigure[In-network.]{\includegraphics[width=2.7cm]{siamese/retall2.png}}
\subfigure[Joint data input.]{\includegraphics[width=2.7cm]{siamese/retall3.png}}
\subfigure[Features with cosine distance.]{\includegraphics[width=2.7cm]{siamese/cosineDistance.png}}
\subfigure[Cnn fine-tuned.]{\includegraphics[width=2.7cm]{siamese/retall2cnnMAss.png}}\\


\caption{Siamese CNN topologies.}
\label{siameseData1}
\end{figure}


The main characteristics of the trained networks are the following:

\begin{itemize}

\item \textbf{Loss}, we used the binary cross entropy as a loss, we tried with the contrastive divergence but it did not converge.

\item \textbf{Optimizer}, As optimizer we used Adam, even though it has a mechanism to decrease the learning rate, we add a exponential decay, it speed up the convergence

\item \textbf{Activation}, we used ReLu. Currently, there other activations functions, but ReLu has been established as the reference.

\item \textbf{Initialization}, To initialize the weights we used He. initialization, in addition we initialize the biases with the value of $0.1$, in this way we avoid the dead neurons in the firsts iterations.

\item \textbf{Batch normalization}, We tested batch normalization, but it adds to much computation time and we discarded it.

\item \textbf{Regularization}, we use Dropout in the fully connected layer to avoid overfitting.


\item \textbf{Final layers}, In the junction between the convolutional layers and the fully connected historically, a flatten mechansihm of the tensor has been used, but it increases dramatically the number of neurons in the fully connected layer, and it shows problems to converge. From the publication of InceptionV3, it appears with a global average pooling layer, it computes the spatial average of each layer of the tensor, reducing the number of parameters. Also we used the spatial pyramid pooling layers, it consists in a multiresolution max pooling. We can observe those differences in ref{siameseData2}.

\begin{figure}[H]
		
\centering

\subfigure[Flatten.]{\includegraphics[width=4.5cm]{siameseDev/flatten2.png}}
\subfigure[Global average pooling.]{\includegraphics[width=4cm]{siameseDev/globalPooling.png}}
\subfigure[Spatial pyramid pooling.]{\includegraphics[width=4cm]{siameseDev/spp2.png}}\\



\caption{Final layers.}
\label{siameseData2}
\end{figure}


\item \textbf{Output}, We did not use softmax as output, we only used one neuron with sigmoid activation, in this way the output is constrained between $0$ and $1$.



\end{itemize}


We developed our models in a VGG way, stacking  several convolutional layers and finishing it with a fully connected layers. We started with a few convolutional layers and added more till we reach an overfitting condition, this conditional will manifest when adding more layers the score on the test set declined. We started with $3$ and we end up with $7$ convolutional layer as best performance.

For the dataset, it does not exist a prominent dataset in the field, so we decided to use the MOT16 as dataset to adapt the domain. In order to do so, we extract the detections with their identities, and then for each identity we selected all possible random pairs and for the negative set we selected several random identities. The negative dataset is much bigger than the positive dataset, so we limited it to have a balanced dataset. The problem with the MOT16 dataset, is that the ground truth was built with the detections of a classifier and there is not a human intervention, resulting in a messy ground truth. We inspected the dataset and around the $70 \%$ of the dataset was wrong, there are a lot of occlusion in detection resulting in erroneous pairs, pairs that are not matching with the same identity.

Then, we decided to discard the MOT16 dataset an use the TownCenter dataset \cite{townCenter} from the University of Oxford, which has got a manual ground truth. We have got $29824$ positive and negative pairs, then a dataset of $59648$ image pairs. We split the dataset between training and validation set, $80 \%$ and $20 \%$ respectively. For testing we selected a set of identities of the MOT16 dataset. To regularize and enlarge our dataset we applied some data augmentation techniques to our dataset like we observe in figure \ref{msii1}. For each pair we added one transformation, so we double our dataset. We tried to apply all the transformation for each images but the dataset was too noisy and the network did not converge.






\begin{figure}[H]
		
\centering

\subfigure[Original image.]{\includegraphics[width=2cm]{dataAugmentation/resizedImage.jpg}}
\subfigure[Random image brightness.]{\includegraphics[width=2cm]{dataAugmentation/imageBrightnes.jpg}}
\subfigure[Random crop.]{\includegraphics[width=2cm]{dataAugmentation/imageRandomCrop.jpg}}
\subfigure[Vertical flip.]{\includegraphics[width=2cm]{dataAugmentation/imageVerticalFlip.jpg}}
\subfigure[Gaussian blur.]{\includegraphics[width=2cm]{dataAugmentation/imageGaussianBlur.jpg}}
\subfigure[Random shadow.]{\includegraphics[width=2cm]{dataAugmentation/imageRandomShadow.jpg}}


\subfigure[Zoom in.]{\includegraphics[width=2cm]{dataAugmentation/imageZoomIn.jpg}}
\subfigure[Rotation and translation.]{\includegraphics[width=2cm]{dataAugmentation/imageTransormed.jpg}}
\subfigure[Zoom out.]{\includegraphics[width=2cm]{dataAugmentation/imageZoomOut.jpg}}
\subfigure[Gaussian noise.]{\includegraphics[width=2cm]{dataAugmentation/imageNoiseGaussian.jpg}}
\subfigure[Opposite vignetting.]{\includegraphics[width=2cm]{dataAugmentation/imageBLURcenter.jpg}}
\caption{Data augmentation.}
\label{msii1}
\end{figure}

We trained all the models and obtained a graphs like the figure \ref{lossesSiam}, we observe that the network converge, it decreases the loss and increases the accuracy, also we observe that tests plots are a little bit noisy, but we considered that the  regularization techniques are enough. We notice that the joint data input outperforms the other siamese configurations, so we increased the number of layers of that architecture, \textit{conv I}, refers to it, with $I$ the number of layers.

\begin{figure}[H]
		
\centering

\subfigure[Losses.]{\includegraphics[width=7.5cm]{siameseDev/loss.png}}
\subfigure[Accuracy.]{\includegraphics[width=7.5cm]{siameseDev/accuracy.png}}\\

\caption{Results training.}
\label{lossesSiam}
\end{figure}



Finally, we can observe the comparision using the CMC measure in the figure \ref{lossesSiam2}, we notice that the siamese network with joint data input with less layers than bigger models like Inception performs better, this remarks the idea of training jointly the feature extractor and the classifier and the need of task specific networks. Also, the siamese network with the configuration joint data input, outperforms the other siamese networks. Among the siamese joint data input, the performance increases till the $6$ convolutional layer architecture, then the $7$ convolutional layers drops.

\begin{figure}[hptb]
\centering         
\includegraphics[width=12cm]{siameseDev/cmc2.png}
\caption{CMC plot.} \label{lossesSiam2}
\end{figure}

Also in the next plot, we observe the performance against the time consumption the siamese network with the joint data input with $7$ convolutional layers gets the best balance between performance and execution time. 

\begin{figure}[hptb]
\centering         
\includegraphics[width=12cm]{siameseDev/graps1.png}
\caption{Performance-timing comparision.} \label{lossesSiam3}
\end{figure}




In the table \ref{tableResultsSiamee}, we can check the difference between the algorithm with and without person reidentification module, with the reidentification we reduce around $24 \%$ the identity switching (ID's).


\begin{table}[H]
\centering

\resizebox{\textwidth}{!}{\begin{tabular}{l|llll|llll|lll|l}
              &  \textbf{GT} & \textbf{MT} & \textbf{PT} & \textbf{ML} & \textbf{FP} & \textbf{FN} & \textbf{IDs} & \textbf{FM} & \textbf{MOTA} & \textbf{MOTP} & \textbf{MOTAL} & \textbf{FPS} \\
\textit{Without reidentification}   & 517         & 12           & 180         & 325         & 13339       & 78929       & 827             & 9.8          & 69.1                & 18.2        \\
\textit{With reidentification}  & 517         & 3          & 127         & 387         & 13373       & 78999       & 618                 & 10.8           & 70.3                   & 15.85          \\
  
\end{tabular}}
\caption{Comparision with reidentification module.}
\label{tableResultsSiamee}
\end{table}


\section{Timing performance}\label{expeEVAL}



As we stated above the mean frame rate of the algorithm with the person re-identification mechanism is $15.86$. In figure \ref{timing1} we can observe a barplot of per frame time consumption of our algorithm.  We notice the peaks each $30$ frames, these belong to the execution of the siamese network and it depends on how many detections without assignment there are.

\begin{figure}[H]
\centering         
\includegraphics[width=0.9\linewidth]{graphicsRearrange/temps/timeGenral.png}
\caption{Barplot of the timming.} \label{timing1}
\end{figure}

Getting a zoom in of the graph \ref{timing2}, we can observe when the object detector execution finishes around the frame $75$, then the execution time of the algorithm decreases, the time of reading the frame remain constant, and the tracking gets a peak after the detection and afterwards decreases this is due it erase trackets with the lost mechanism. 

\begin{figure}[H]
\centering         
\includegraphics[width=0.9\linewidth]{graphicsRearrange/temps/timeSpecfici.png}
\caption{Zoom in of the barplot.} \label{timing2}
\end{figure}

To optimize our code we studied how to reduce the execution time of the tracking module, plotting the execution time against some dependent variables, in this case, the number of points and the size of the ROI, as we can observe in figure \ref{timing2}. We notice that the execution time is high correlated with the size of the pedestrian's ROI and not with the number of points. The main responsible is the OpenCV's routine \texttt{calcOpticalFlowPyrLK()}, but we could not modify this parameter, and reducing the number of points would have a remarkable importance in the execution time. 



\begin{figure}[H]
		
\centering

\subfigure[Points vs time.]{\includegraphics[width=7.2cm]{graphicsRearrange/points/points1.png}}
\subfigure[Size ROI vs time.]{\includegraphics[width=7.2cm]{graphicsRearrange/points/timss.png}}
\caption{Time versus points and size of the ROI.}
\label{timing2}
\end{figure}



