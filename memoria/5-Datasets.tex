\chapter{Datasets and evaluation procedures}\label{cap.dataset}


Next we explain the datasets and measures utilized in this thesis.

\section{Datasets for object detection}

This section describes the most common datasets used in object detection tasks. Throughout the history of computer vision research datasets have played a critical role.  They not only provide a means to train and evaluate algorithms, they drive research in new and more challenging directions. In order to accomplish this, they provide:


\begin{itemize}

\item a collection of challenging images and high quality annotation.

\item an standard evaluation methodology, so the performance of the algorithms can be compared. 


\end{itemize}



In the next subsections, we will explain the main datasets for object detection task.


\subsection{Pascal Visual Objects Classes}

The Pascal Visual Object Classes (VOC) challenge  \cite{voc07} is a benchmark in visual object category recognition and detection. Organised annually from $2005$ to $2012$, the challenge and its associated dataset has become accepted as one of the landmark benchmark for object detection. All the images are from the flickr consumer photographs website and annotated with the Mechanical turk tool.

The most popular editions of the challenge for object detection are those from years $2007$ and $2012$.

\subsection{VOC07}

%The VOC2007 dataset consists of annotated consumer photographs collected from %the flickr website. In addition, 

The challenge of the year 2007 contains ten thousand images in the trainval and test sets, with almost twelve thousand objects. This was one the first datasets for object detection before the era deep learning. Also, it is very useful for researchers, due it has $2.5$ mean object per image and it is very challenging. In the figure \ref{data07} we can observe the distribution of images and objects instances. 

\begin{figure}[hptb]
\centering         
\includegraphics[width=0.7\linewidth]{datasets/log1.png}
\caption{Distribution of VOC07 dataset.} \label{data07}
\end{figure}



\subsection{VOC12}

The 2012's edition is also, one of the most used dataset in object detection tasks. It increases the volume of images of the 2007 edition up to ten thousand images on trainval and test set and similar quantity of instances per image. 


\subsection{ImageNet}



ImageNet project \cite{imagenet} with the challenge ImageNet Large Scale Visual Recognition Challenge [ILSVRC] was the first large-scale database, temporally developed to supply the deep learning techniques, eager of feed with tons of images. ImageNet aims to populate the majority of the 80000 synsets of WordNet with an average of 500-1000 clean and full resolution images. The collection was based on the query of that words on several image search engines and human refined on the Amazon Mechanical Turk platform.

In 2016, the project collects more than 10 million of annotated images with 1000 classes. Although its main purpose is image classification, it has an object detection challenge with $200$ categories with over a 1 million images with objects annotated.







\subsection{COCO}

The Microsoft Common Objects in Context also known as COCO dataset \cite{coco}, is a dataset that address the three core research problems in scene understanding:


\begin{itemize}

\item detecting non-iconic views of objects, for many dataset most of the objects have an iconic representation, they appear unobstructed , near the center of the photo and with their canonical shape. So in this dataset, they included images to struggle the object recognition task, like objects in the background, partially occluded, amid clutter. Therefore, reflecting the composition of actual everyday scenes.

\item contextual reasoning between objects, nowadays natural images contain multiple objects, and their identity can only be solved using context, due to small size or ambiguos appearance in the image, so in this dataset, images contain scenes rather isolated objects. 

\item the precise 2D localization of objects, also the detailed spatial understanding of object layout will be a core component of an image understanding system, so this dataset struggle to do so.



\end{itemize}


So, the three main tasks of this challenge are object classification, object detection and semantic scene labelling. This dataset contains 91 object categories, with 2.5 million labelled object instances in 328 thousand images, labeled with the Amazon Mechanical Turk tool.

%
%\subsubsection{Dataset comparison}
%
%The table \ref{dataset0} summarizes the main statistics of the datasets and the figure \ref{diagrama2} the ratio of instances per images of the sets.
%
%The datasets from Pascal challenge are very useful to test object detection algorithms, their quantity is very handy ( a few thousands of images ) and contains a challenging quantity of objects per image, very interesting for the algorithms. But it little amount of images doesn't permit to train a network on this dataset, although it can be used to finetune the network.
%
%The datasets for the ImageNet challenge are not used to much in object detection tasks, it contains a few instances per image, this not encourage researcher to use it. Although, is very utilize to extract features and then finetune your net.
%
%
%The COCO datasets, is the most recent one, is the one focus on object recognition, and the detection suppose a challenge due the objects are in common places and are very challenging to detect. And it is very interesting to due of the quantity of instances per image.
%
%
%\begin{table}[H]
%\centering
%
%\begin{tabular}{lllll}
%                                 & \textbf{VOC07} & \textbf{VOC12} & \textbf{ImageNet [ 2014 ]} & \textbf{Coco [ 2015 ]} \\
%\textit{trainval set}            & 5011           & 11540          & 476688                     & 165482                 \\
%\textit{test set}                & 4952           & 10991          & 40152                      & 81434                  \\
%\textit{Number of classes}       & 20             & 20             & 200                        & 80                     \\
%\textit{Mean obj per image}      & 2.5            & 2.4            & 1.1                        & 7.2                    \\
%\textit{Number person instances} & 4690           & 8566           & -                          & 300000                
%\end{tabular}
%\caption{Datasets tables}
%\label{dataset0}
%\end{table}
%
%
%\begin{figure}[H]
%\centering         
%\includegraphics[width=0.7\linewidth]{datasets/genral3.png}
%\caption{Distribution of pascal.} \label{diagrama2}
%\end{figure}
%
%
%
%\begin{figure}[H]
%		
%\centering
%\subfigure[Pascal VOC.]{\includegraphics[width=5.2cm]{datasets/pascal.jpg}}
%\subfigure[ImageNet.]{\includegraphics[width=5.2cm]{datasets/imagenet.jpg}}
%\subfigure[COCO.]{\includegraphics[width=5.2cm]{datasets/coco.jpg}}
%
%\label{iconic}
%\end{figure}
%


\subsection{Dataset comparison}

%The table \ref{dataset0} summarizes the main statistics of the datasets and the figure %\ref{diagrama2} the ratio of instances per images of the sets.

The datasets from Pascal challenge are very useful to test object detection algorithms, their quantity is very handy ( a few thousands of images ) and contains a challenging quantity of objects per image, very interesting for the algorithms. But its little amount of images does not permit to train a network on this dataset, although it can be used to finetune the network.

The datasets for the ImageNet challenge are not used to much in object detection tasks, it contains a few instances per image, this did not encourage researcher to use it.


The COCO datasets, is the most recent one, is the one focus on object recognition, and the detection suppose a challenge due the objects are in common places and are very challenging to detect. And it is very interesting to due of the quantity of instances per image.


The COCO challenge contains 91 object categories with 82 of them having more than 5 thousand labeled instances. In total the dataset has 2.5 million labeled instances in 328 thousand images. In contrast to ImageNet dataset, COCO has fewer categories but more instances per category. Also, it has more instances per category than the VOC dataset. We can observe that difference in the chart \ref{instancesCategory}. This fact aid in learning detailed object models capable to chance the variability and also its 2D location.

\begin{figure}[H]
\centering         
\includegraphics[width=0.7\linewidth]{datasets/sdad2.png}
\caption{Distribution of pascal.} \label{instancesCategory}
\end{figure}

In addition, another prominent feature of the COCO over the other two, is the number of labelled instances per image which may aid in learning contextual information. This difference can be seen in \ref{instancesImage} chart.

%\begin{figure}[H]
%\centering         
%\includegraphics[width=0.7\linewidth]{datasets/instancesPerImage.png}
%\caption{Distribution of pascal.} \label{instancesImage}
%\end{figure}

Moreover, the COCO dataset uses images from non-canonical point view, allowing to the algorithm to be robust to everyday views. This feature can be observed in the plot \ref{iconic}, in this plot we can observe differences views of the same category. And clearly the coco's images is the most uni-conic representation.

\begin{figure}[H]
		
\centering
\subfigure[Pascal VOC.]{\includegraphics[width=5.2cm]{datasets/pascal.jpg}}
\subfigure[ImageNet.]{\includegraphics[width=5.2cm]{datasets/imagenet.jpg}}
\subfigure[COCO.]{\includegraphics[width=5.2cm]{datasets/coco.jpg}}
\caption{Distribution of pascal.} \label{iconic}

\end{figure}


Finally, the table \ref{dataset0} summarizes the main statistics of the dataset stated previously.

\begin{table}[H]
\centering

\begin{tabular}{lllll}
                                 & \textbf{VOC07} & \textbf{VOC12} & \textbf{ImageNet [ 2014 ]} & \textbf{Coco [ 2015 ]} \\
\textit{trainval set}            & 5011           & 11540          & 476688                     & 165482                 \\
\textit{test set}                & 4952           & 10991          & 40152                      & 81434                  \\
\textit{Number of classes}       & 20             & 20             & 200                        & 80                     \\
\textit{Mean obj per image}      & 2.5            & 2.4            & 1.1                        & 7.2                    \\
\textit{Number person instances} & 4690           & 8566           & -                          & 300000                
\end{tabular}
\caption{Datasets tables}
\label{dataset0}
\end{table}


\subsection{Evaluation of object detection algorithms}

In order to compare the performance of the different datasets, each challenges establish a clear measure. In this thesis, we used the interpolated average precision (AP), used in the Pascal VOC challenge (based on \cite{salton}).

For each class, the precision-recall curve is computed from a method's ranked output.

\begin{itemize}

\item Recall, is defined as the proportion of all positives examples ranked above a given rank.

\item Precision is the proportion of all examples above the rank which are from the positive class.

\end{itemize}


The AP summarises the shape of the precision/recall curve, and is defined as the mean precision at a set of eleven equally spaced recall levels [0,0.1,...,1]:

$$ AP = \dfrac{1}{11} \sum_{r \epsilon (0,...,1)} p_{interp}(r) $$

The precision at each recall level $r$ is \textit{interpolated} by taking the maximum precision measured for a method for which the corresponding recall exceeds $r$:

$$ p_{interp}(r) = max_{\hat r: \hat r>r} p( \hat r)$$

The authors justified this measurement as a way to reduce the impact of the 'wiggles' in the precision/recall curve, caused by small variations in the ranking of examples. In the figure \ref{diagramaI}, we can observe this effect on the curve.

\begin{figure}[H]
\centering         
\includegraphics[width=0.7\linewidth]{evaluacionObject/interpol.png}
\caption{Comparision interpolated and normal curve.} \label{diagramaI}
\end{figure}


In addition, detections were assigned to ground truth objects and judged to be true/false positives by measuring bounding box overlap. To be considered a correct detection, the area of overlap $a_{0}$ between the predicted bounding box $B_{p}$ and ground truth bounding box $ B_{gt}$ must exceed 0.5 by the formula:


$$ a_{0} = \dfrac{area(B_{p} \cap B_{gt})}{area(B_{p} \cup B_{gt})} $$

where $B_{p} \cap B_{gt}$ denotes the intersection of the predicted and ground truth bounding boxes and $ B_{p} \cup B_{gt} $ their union. The treshold of 50 \%  was set deliberately low to account for inaccuracies in bounding boxes in the ground truth data. Multiple detections of the same object in an image were considered false detections.

Finally, we want to point out, even we don't take into account in our implementation,  setting the threshold IoU to a value of $0.5$ could cause misdetections of small objects \cite{imagenet}, they propose an adaptive setting of that threshold based on the size of the ground truth and so detect correctly small objects. In practice this change only affects $5.5\%$ of objects in the detection validation set.








\section{Datasets for multiple object tracking}

Evaluating and comparing multi-target tracking methods is not trivial for numerous reasons. 

\begin{itemize}

\item First, the perfect solution one aims to achieve is difficult to define clearly. Partially visible, occluded, or cropped targets, reflections, and objects that are very closely resemble targets; all impose intrinsic ambiguities, such that even humans may not agree on one particular ideal solution.

\item Second, a number of different evaluation metrics with free parameters and ambiguous definitions often lead to inconsistent quantitative results across the literature.

\item Finally, the lack of pre-defined test and training data makes it difficult to compare different methods fairly.

\end{itemize}


In contrast to other research areas in computer vision, still lacks large-scale benchmarks.

\subsection{PETS}

Targeted primarily at surveillance applications \cite{pets}. The 2009 version consisted of 3 subsets, S1 targeted at person count and density estimation, S2 targeted at people tracking, and S3 targeted at flow analysis and event recognition. In the \ref{petsExample} we can observe one image from this dataset.

\begin{figure}[H]
\centering         
\includegraphics[width=0.5\linewidth]{datasetTracking/View_001.jpg}
\caption{Example of Pets.} \label{petsExample}
\end{figure}

Even for this widely used benchmark, we observe that tracking results are commonly obtained in an inconsistent fashion: involving using different subsets of available data, different detection inputs, inconsistent model training that is often prone to over-fitting, and varying evaluation scripts. Results are thus not easily comparable \cite{mot}.


\subsection{MOT challenge}

The aim of the Multiple object tracking [MOT] is to standardize the use of multiple people trackings datasets, in order to do so, they solve the problems in this kind of dataset, explained above.


\subsection{Evaluation of multiple people tracking algorithms}

A critical point with any dataset is how to measure the performance of the algorithms. A large number of metrics for quantitative evaluation of multiple target tracking have been proposed. Choosing unique general evaluation is still ongoing. 

On the one hand, it is desirable to summarize the performance into one single number to enable a direct comparison. On the other hand, one might not want to lose information about the individual errors made by the algorithms and provide several performance estimates, which precludes a clear ranking.


We will explain two sets of measures that have established themselves in the literature the CLEAR metrics \cite{clear}, and a set of track quality measures \cite{wu}.

As in the object detection metrics, we can classify each tracket, whether it is a true positive, that describes an actual (annotated) target, whether the output is a false alarm ( or false positive, FP ). This decisions is typically made by the well-known thresholding measure of Intersection of the union [IoU]. Also a target that is mised by any tracker is a false negative.

Due to we are working with multiple object, we assume that each ground truth trajectory has one unique start and one unique end point, that is not fragmented. So we need to penalty re-identification. This is called, identity switch [IDSW], and it is counted as if a ground truth target $i$ is matched to track $j$ and the last known assignment was $ k = j$. The next figure summarizes the stated measures ( the grey area indicate the the matching threshold ).


\begin{figure}[H]
\centering         
\includegraphics[width=0.7\linewidth]{datasetTracking/trackins.png}
\caption{Example of measures.} \label{petsExample}
\end{figure}

Then after determining true matches and establishing the correspondandes it is now possible to compute the metrics over all the sequences.


The multiple object tracking accuracy [MOTA] \cite{clear} is perhaps the most widely used figure to evaluate a tracker's perfomance. The main reason for this is its expressiveness as it combines as it combines three sources of errors defined above:

$$ MOTA = 1 - \frac{\sum_{t} (FN_{t}+FP_{t}+IDSW_{t})}{ \sum_{t} GT_{t}}$$

where $t$ is the frame index and GT is the number of ground truth objects. This measures gives an indication of the overall performance.

The multiple object tracking precision [MOTP] is the average dissimilarity between all true postives and their corresponding ground truth targets. For bounding box overlap, that is computed as 

$$ MOTP =  \frac{\sum _{t,i} d_{t,i}}{ \sum_{t} c_{t}} $$

where $c_{t}$ denotes the number of matches in frame $t$ and $d_{t,i}$ is the bounding box overlap of target $i$ with its assigned ground truth object. Thereby gives the average overlap between all correctly matched hypotheses. So, the MOTP is a measure of localization precision.


As we have stated above, another metrics are the track quality measures. Each ground truth trajectory can be classified as mostly tracked (MT), partially tracked (PT), and mostly lost (ML). This is done based on how much of the trajectory is recovered by the tracking algorithm. A target is mostly tracked if it is successfully tracked for at least $80 \%$ of its life span, without consider if there are an identity switch. If a track is only recovered for less than $20 \%$ of its total length, it is said to be mostly lost (ML). All other tracks are partially tracked. Finally antoher quality measure is track fragmentations (FM), it counts how many times a ground truth trajectory is resumed at a later point.


\section{Datasets for pedestrian identification}

A number of datasets for image-based re-Identification have been released, and some commonly used datasets are summarized in table \ref{tableID}. 


\begin{table}[H]
\centering

\begin{tabular}{lllllll}
\textbf{Name}        & \textbf{Date} & \textbf{Images} & \textbf{IDs} & \textbf{Cameras} & \textbf{Label} & \textbf{Evaluation} \\
\textit{VIPeR} \cite{viper}      & 2007          & 1264                      & 632          & 2                & hand           & CMC                 \\
\textit{iLIDS}  \cite{lids}      & 2009          & 476                       & 119          & 2                & hand           & CMC                 \\
\textit{GRID}   \cite{grid}     & 2009          & 1275                      & 250          & 8                & hand           & CMC                 \\
\textit{CAVIAR} \cite{caviar}      & 2011          & 610                       & 72           & 2                & hand           & CMC                 \\
\textit{PRID2011} \cite{prod11}   & 2011          & 1134                      & 200          & 2                & hand           & CMC                 \\
\textit{WARD}   \cite{ward}     & 2012          & 4786                      & 70           & 3                & hand           & CMC                 \\
\textit{CUHK01} \cite{cuk1}     & 2012          & 3884                      & 971          & 2                & hand           & CMC                 \\
\textit{CUHK02}  \cite{cuk2}    & 2013          & 7264                      & 1816         & 10               & hand           & CMC                 \\
\textit{CUHK03}  \cite{cuk3}    & 2014          & 13164                     & 1467         & 2                & hand/DPM       & CMC                 \\
\textit{RAiD}  \cite{raid}      & 2014          & 1264                      & 43           & 4                & hand           & CMC                 \\
\textit{PRiD 450S} \cite{prid450}   & 2014          & 900                       & 450          & 2                & hand           & CMC                 \\
\textit{Market-1501} \cite{market} & 2015          & 32668                     & 1501         & 6                & hand/DPM       & CMC/mAP            
\end{tabular}
\caption{Statistical comparision datasets.}
\label{tableID}
\end{table}

Over recent years, progress can observed, dataset's scale is increasing. Many of these datasets are relatively small in size, especially those of early days, but recent datasets, such as CUHK03 and Market-1501, are larger. Both have over 1000 ID's and over 10000 bounding boxes, and both datasets provide good amount of data for training deep learning models. In adition, the bounding boxes tend to be produced by pedestrian detectors, instead of being hand-drawn. Also, more cameras are used during collection, this helps to increase generalization. Altough quantity of datasets, there are not a prominent dataset in the literature.

\subsection{Evaluation for pedestrian identification}

% http://ws680.nist.gov/publication/get_pdf.cfm?pub_id=50759

When evaluating identification algorithms, the cumulative matching characteristics (CMC) curve is usually used. CMC represents the probability that a query identity appears in differentsized candidate lists.

Formally \cite{faceCMC}, for each probe $p$ from $P_{G}$ we sort the similarity scores against gallery $G$, and obtain the rank of the match. Identification performance is then stated as the fraction of probes whose gallery match it at rank $r$  or lower. If the set of probes with a close match is:

$$ C(r) = \big\{ p_{j}: rank(p_{j}) \leq r  \bigr\} \hspace{0.3cm} \forall p_{j} \in P_{G}  $$

where the rank is defined as before. We now define the Cumulative match characteristic (CMC) to be the identification rate as a function of $r$:

$$ P_{I}(r) = \dfrac{\abs{C(r)}}{\abs{P_{G}}} $$

which we plot as the primary measure of identification performance. It gives an estimate of the rate at which probe images will be classified at rank $r$ or better. One drawback of the characteristics is its dependence on gallery size, $\abs{G}$.